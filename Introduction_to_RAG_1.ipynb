{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavun-KumarCH/Agentic-RAG-Systems/blob/main/Introduction_to_RAG_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mp0jwJeAMWs"
      },
      "source": [
        "# Rag From Scratch: Overview\n",
        "These notebooks walk through the process of building RAG app(s) from scratch.\n",
        "\n",
        "They will build towards a broader understanding of the RAG langscape, as shown here:\n",
        "\n",
        "![Local Image](./assets/RAG-intro.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kaaZfG7ah2YS"
      },
      "outputs": [],
      "source": [
        "#@title requirements\n",
        "%pip install --q langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Av-zWn8jiMTX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvKDR-JhjZT7"
      },
      "source": [
        "# Part 1 : Overview\n",
        "\n",
        "[RAG](https://python.langchain.com/docs/tutorials/rag/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5zHEhG7jbkK",
        "outputId": "3b7ff68c-7a56-437c-bf06-8c3e14674915"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "# Load Dependencies\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from IPython.display import Markdown\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "FzA8Qo9ckeyt",
        "outputId": "a4b095ef-58e9-4f29-e0d9-2206d458fed2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The approaches to Task Decomposition include using LLM with simple prompting, task-specific instructions, and human inputs. Task decomposition involves breaking down a problem into smaller steps to make it more manageable and easier to solve. Different methods can be used to guide the decomposition process, such as providing specific instructions or using language models for prompting."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#### Indexing ###\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs = dict(\n",
        "        parse_only = bs4.SoupStrainer(\n",
        "            class_ = (\"post-content\", \"post-title\",\"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "## Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "## Embed\n",
        "vectorstore = Chroma.from_documents(splits, embedding = OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "## LLM\n",
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n",
        "                 temperature = 0.2,\n",
        "                 top_p = 0.7)\n",
        "\n",
        "## Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "## RAG pipeline Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\":\n",
        "      RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "## Question\n",
        "Markdown(rag_chain.invoke(\"What are the approaches to Task Decomposition?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMM4Ixh_ngIW"
      },
      "source": [
        "# Part-2 : Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Local Image](./assets/indexing.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IRMB1HFAnf0b"
      },
      "outputs": [],
      "source": [
        "# Documents\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "document = \"My Favorite pet is a cat.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBbf8sKkobrR"
      },
      "source": [
        "* Count tokens considering ~4 char / token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyakYjh6nvVW",
        "outputId": "6211e6a9-375d-4bc8-9867-1e543cb5ea4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokents_from_string(string: str, encoding_name: str) -> int:\n",
        "  \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "  encoding = tiktoken.get_encoding(encoding_name)\n",
        "  num_tokens = len(encoding.encode(string))\n",
        "  return num_tokens\n",
        "\n",
        "num_tokents_from_string(question, \"cl100k_base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKEEougnohFk"
      },
      "source": [
        "* Text embedding models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBxoXNvMoijs",
        "outputId": "204d6c9c-f0d0-422b-e3db-dcdf6a962ba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "query_result = embeddings.embed_query(question)\n",
        "document_result = embeddings.embed_query(document)\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_W8wZ74oyig"
      },
      "source": [
        "* Cosine similarity is reccomended (1 indicates identical) for OpenAI embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puDPq_aWowuh",
        "outputId": "b84c77fa-248d-480f-af88-26c695b93120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.6775632124372357\n"
          ]
        }
      ],
      "source": [
        "# Sematic Search metric Cosine Similarity\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1,vec2):\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  norm_vec1 = np.linalg.norm(vec1)\n",
        "  norm_vec2 = np.linalg.norm(vec2)\n",
        "  return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc3lgCdmpgQU"
      },
      "source": [
        "* Document Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "REkzu3l5piBx"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs = dict(\n",
        "        parse_only = bs4.SoupStrainer(\n",
        "            class_ = (\"post-content\", \"post-title\",\"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUZC5k3Pp92j"
      },
      "source": [
        "* splitter\n",
        "\n",
        "\n",
        "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tCRzXExip7hm"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        ")\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYOHc85Fqe_3"
      },
      "source": [
        "* Vectorstores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Uq6va-27qgSM"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstors = Chroma.from_documents(splits, embedding = OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstors.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG0B6L7_q7bp"
      },
      "source": [
        "# Part-3 : Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jQOTM1dpq_Py"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstor = Chroma.from_documents(documents = splits,\n",
        "                                    embedding = OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstor.as_retriever(search_kwargs = {\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "ItTOxHBErfjg",
        "outputId": "9e5610e6-75e5-4983-99b6-dc1ed997c279"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-768e2c551f1d>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# relevant douments search\n",
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
        "display(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8XD4GsbslY2"
      },
      "source": [
        "# Part 4 : Generation\n",
        "\n",
        "![Local Image](./assets/generation.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v97dMtWmrs1E",
        "outputId": "e52f22ba-b3f5-4271-b773-244f8d6870ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nAnswer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ANh9hVOtENV",
        "outputId": "a10e0c2c-2254-414b-dbd4-31f539123d57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps, making them more manageable for autonomous agents. This process involves transforming big tasks into multiple manageable tasks, allowing agents to plan ahead and execute tasks more effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 672, 'total_tokens': 720, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65009b76-06fd-422b-a6e2-f08bd33c73e0-0', usage_metadata={'input_tokens': 672, 'output_tokens': 48, 'total_tokens': 720, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n",
        "                 temperature = 0.2,\n",
        "                 top_p = 0.7)\n",
        "\n",
        "# RAG pipeline Chain\n",
        "chain = prompt | llm\n",
        "\n",
        "# Run\n",
        "question = \"What is Task Decomposition?\"\n",
        "chain.invoke({\"context\": docs, \"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "yZORWYdJtWWp",
        "outputId": "a0371efb-834d-4c1d-c6e4-811cdc9aff20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "# Prompt\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "display(prompt_hub_rag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHr7omRt1n0"
      },
      "source": [
        "* [RAG Cains](https://python.langchain.com/docs/how_to/sequence/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "5DQooBnCtd1n",
        "outputId": "c39b9077-be2a-4686-bd3f-2c2ee6a18340"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The approaches to Task Decomposition include using LLM with simple prompting, task-specific instructions, or human inputs. Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, decomposing the problem into multiple thought steps and generating a tree structure. The search process can be BFS or DFS with each state evaluated by a classifier or majority vote."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\":\n",
        "      RunnablePassthrough()}\n",
        "    | prompt_hub_rag\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "Markdown(rag_chain.invoke(\"What are the approaches to Task Decomposition?\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM8Ws4sNYR9GEANOrI+RxZo",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
