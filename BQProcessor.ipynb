{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPwFnHhy7ETJtzR+NJkzHNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavun-KumarCH/Agentic-RAG-Systems/blob/main/BQProcessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title requirements\n",
        "%pip install --q langchain_groq langchain_core"
      ],
      "metadata": {
        "id": "W8JAfyqxr05y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rmi_arDl9pQ"
      },
      "outputs": [],
      "source": [
        "#@title Libraries\n",
        "import os, time, queue, threading\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Environment\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initializing Mistral:7B Model\n",
        "\n",
        "llm = ChatGroq(\n",
        "  model = \"gemma2-9b-it\",\n",
        "  temperature = 0.4,\n",
        "  max_tokens = None,\n",
        "  timeout = None,\n",
        "  max_retries = 2\n",
        ")"
      ],
      "metadata": {
        "id": "eEyeqcpPnBME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prompt\n",
        "# Prompt Template for Customer Service Assistant\n",
        "prompt_template = \"\"\"Customer Support Conversation:\n",
        "{conversation}\n",
        "AI: You are a chatbot simulating a customer support assistant. Provide helpful, concise answers to customer inquiries without unnecessary details.\n",
        "\"\"\"\n",
        "# PromptTemplate Instance\n",
        "chat_prompt = PromptTemplate(\n",
        "  template = prompt_template,\n",
        "  input_variables = [\"conversation\"]\n",
        ")"
      ],
      "metadata": {
        "id": "5tTDD2YgnlPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Class Batch Processor\n",
        "class BatchProcessor:\n",
        "  def __init__(self, processor_id: int, batch_size: int, batch_interval: float):\n",
        "    self.processor_id = processor_id\n",
        "    self.batch_size = batch_size\n",
        "    self.batch_interval = batch_interval\n",
        "    self.request_queue = queue.Queue()\n",
        "    self.lock = threading.Lock()\n",
        "    self._stop_event = threading.Event()\n",
        "    self.worker_thread = None\n",
        "\n",
        "# Start Batching\n",
        "  def start_batching(self):\n",
        "    if self.worker_thread is None or not self.worker_thread.is_alive():\n",
        "      self._stop_event.clear()\n",
        "      self.worker_thread = threading.Thread(target = self.batch_worker)\n",
        "      self.worker_thread.start()\n",
        "      print(f\"Batch processor {self.processor_id} started batching.\")\n",
        "\n",
        "# Add Request\n",
        "  def add_request(self, request_data: Dict[str, str]):\n",
        "    self.request_queue.put(request_data)\n",
        "    print(f\"{request_data}Request added to processor {self.processor_id}.\")\n",
        "\n",
        "# Process Batch\n",
        "  def process_batch_with_langchain(self, batch: List[Dict[str, str]]):\n",
        "    formatted_prompts = [\n",
        "        chat_prompt.format(conversation = \"\\n\".join(f\"{item['role'].capitalize()}: {item['content']}\" for item in [request]))\n",
        "        for request in batch\n",
        "    ]\n",
        "\n",
        "    # Batch Method\n",
        "    responses = llm.batch(formatted_prompts)\n",
        "    for response in responses:\n",
        "      print(f\"Response from processor {self.processor_id}: {response}\")\n",
        "      print(response, \"\\n\")\n",
        "\n",
        "# Batch Worker\n",
        "  def batch_worker(self):\n",
        "    while not self._stop_event.is_set():\n",
        "      batch = []\n",
        "      start_time = time.time()\n",
        "      while len(batch) < self.batch_size and (time.time() - start_time) < self.batch_interval:\n",
        "        try:\n",
        "          request_data = self.request_queue.get(timeout = self.batch_interval - (time.time() - start_time))\n",
        "          batch.append(request_data)\n",
        "          self.request_queue.task_done()\n",
        "        except queue.Empty:\n",
        "          break\n",
        "\n",
        "      if batch:\n",
        "        print(f\"Processor {self.processor_id} - Processing batch of size {len(batch)}\")\n",
        "        with self.lock:\n",
        "          self.process_batch_with_langchain(batch)\n",
        "        print(f\"Processor {self.processor_id} - Batch processor complete.\\n\")\n",
        "      else:\n",
        "        print(f\"Processor {self.processor_id} - No requests  to process in the cycle.\")\n",
        "\n",
        "\n",
        "# Stop Batching\n",
        "  def stop_batching(self):\n",
        "    self._stop_event.set()\n",
        "    if self.worker_thread:\n",
        "      self.worker_thread.join()\n",
        "      self.worker_thread = None\n",
        "      print(f\"Batch processor {self.processor_id} stopped batching.\")"
      ],
      "metadata": {
        "id": "k6ZgoeTDoZb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Class Load Balancer -> round robin distribution\n",
        "class LoadBalancer:\n",
        "  def __init__(self, processors:List['BatchProcessor']):\n",
        "    self.processors = processors\n",
        "    self.current_processor_index = 0\n",
        "\n",
        "  def distribute_request(self, request_data: Dict[str, str]):\n",
        "    processor = self.processors[self.current_processor_index]\n",
        "    processor.add_request(request_data)\n",
        "    self.current_processor_index = (self.current_processor_index + 1) % len(self.processors)\n",
        "\n",
        "\n",
        "# Chat Interactions as Incoming Requests\n",
        "chat_interactions = [\n",
        "  {\"role\": \"user\", \"content\": \"Hello, I need help with my account login issues.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Can you tell me about the latest offers on premium memberships?\"},\n",
        "  {\"role\": \"user\", \"content\": \"My last payment didn't go through. Can you help me resolve this?\"},\n",
        "  {\"role\": \"user\", \"content\": \"How can I update my contact details in my profile?\"},\n",
        "  {\"role\": \"user\", \"content\": \"Can you suggest the best plan for a small business?\"},\n",
        "  {\"role\": \"user\", \"content\": \"I forgot my password. Can you help me reset it?\"},\n",
        "  {\"role\": \"user\", \"content\": \"I'm having trouble with the app crashing frequently.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Can I cancel my subscription? If so, how?\"},\n",
        "  {\"role\": \"user\", \"content\": \"I need help understanding the billing statement I received.\"},\n",
        "  {\"role\": \"user\", \"content\": \"How do I enable notifications for account updates?\"}\n",
        "]\n",
        "\n",
        "# Initializing two batch processors\n",
        "batch_processor1 = BatchProcessor(processor_id = 1, batch_size = 2, batch_interval = 10)\n",
        "batch_processor2 = BatchProcessor(processor_id = 2, batch_size = 2, batch_interval = 10)\n",
        "\n",
        "# Initializing load balancer with processors\n",
        "load_balancer = LoadBalancer(processors = [batch_processor1, batch_processor2])\n",
        "\n",
        "# Starting batching for both processors\n",
        "batch_processor1.start_batching()\n",
        "batch_processor2.start_batching()\n",
        "\n",
        "# Adding each meassge to the load balancer with a timestamp\n",
        "for interaction in chat_interactions:\n",
        "  interaction_with_timestamp = interaction.copy()\n",
        "  interaction_with_timestamp[\"timestamp\"] = datetime.now().isoformat()\n",
        "  load_balancer.distribute_request(interaction_with_timestamp)\n",
        "  time.sleep(1)\n",
        "\n",
        "# Stopping batching for both processors\n",
        "batch_processor1.stop_batching()\n",
        "batch_processor2.stop_batching()"
      ],
      "metadata": {
        "id": "KURQpLZqrom-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}